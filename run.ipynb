{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sre_parse import expand_template\n",
    "import time\n",
    "from read_maze import get_local_maze_information\n",
    "from read_maze import load_maze\n",
    "\n",
    "import numpy as np\n",
    "from maze_env import Maze\n",
    "from environment import Environment\n",
    "from agent import DeepQNetwork\n",
    "from CNNagent import CNNDeepQNetwork\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "import sys\n",
    "class Logger(object):\n",
    "    def __init__(self, filename='H:\\DLwork\\DQN_work\\\\default.log', stream=sys.stdout):\n",
    "        self.terminal = stream\n",
    "        self.log = open(filename, 'w')\n",
    " \n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    " \n",
    "    def flush(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "sys.stdout = Logger('H:\\DLwork\\DQN_work\\\\default.log', sys.stdout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "    episodes = 1000\n",
    "\n",
    "    n_action = 5\n",
    "    #Dimension of input neural network\n",
    "    n_feature = 36  \n",
    "    #Number of hidden neurons\n",
    "    n_hidden =512\n",
    "    pre_postion = (1,1)\n",
    "    # canv = Maze()\n",
    "    agent = CNNDeepQNetwork(n_action, n_feature,n_hidden,\n",
    "                learning_rate=0.01,\n",
    "                gamma_rate=0.9,\n",
    "                epsilon_greedy=0.9,\n",
    "                q_target_replace=500,\n",
    "                memory_size=10000,\n",
    "                batch_size=128,\n",
    "                greedy_flag=True\n",
    "                )\n",
    "    env = Environment()\n",
    "\n",
    "    print('...starting...')\n",
    "    \n",
    "\n",
    "    for train_episodes in range(episodes):\n",
    "\n",
    "    #Initializing the map\n",
    "        \n",
    "        observation = env.reset()\n",
    "        # canv.set_dynamic(env.get_original_around, env.get_actor_postion, [])\n",
    "        done = False\n",
    "        #Initialize the maze environment\n",
    "        last_epoch_postion = pre_postion\n",
    "        \n",
    "        # observation = np.zeros((2,1))\n",
    "\n",
    "        score = 0\n",
    "        step = 0\n",
    "        ##Random explore\n",
    "        # while not done:\n",
    "        #     canv.maze_run(env.get_original_around, env.get_actor_postion, env.get_actor_path)\n",
    "\n",
    "        #     action = np.random.randint(0,4)\n",
    "        #     observation_, reward, done = env.step(action, score)            \n",
    "        #     agent.store_memory(observation, action,reward, observation_)\n",
    "            \n",
    "        # print(\"finish explore\")\n",
    "\n",
    "\n",
    "        \n",
    "        same_po = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            # canv.maze_run(env.get_original_around, env.get_actor_postion, env.get_actor_path)\n",
    "            #Choose action base on epsion-greedy\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done = env.step(action, score)\n",
    "\n",
    "            #If agent has not changed position for too long\n",
    "            if env.get_actor_postion == pre_postion:\n",
    "                same_po +=1\n",
    "            else: \n",
    "                same_po = 0\n",
    "            if same_po>50:\n",
    "                done = True\n",
    "                print(\"Stay too long\")\n",
    "\n",
    "\n",
    "\n",
    "#Out put\n",
    "            obs_origin = env.get_original_around\n",
    "            wall = []\n",
    "            fire = []\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if obs_origin[i][j][0] == 0:\n",
    "                        wall.append([i,j])\n",
    "                    if obs_origin[i][j][1] >0: \n",
    "                        fire.append([i,j])   \n",
    "\n",
    "            # print(\"Time\",step,\"actor position\",env.get_actor_postion,\"action\",action)\n",
    "            # print(\"Path\",env.get_actor_path)\n",
    "            # print(\"Observation\")\n",
    "            # print(\"Wall\",wall)\n",
    "            # print(\"fire\",fire)\n",
    "            # print(\" \")\n",
    "            score += reward\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            #record position in last step\n",
    "            pre_postion = env.get_actor_postion\n",
    "            #store into memory\n",
    "            agent.store_memory(observation, action,reward, observation_)\n",
    "            if (step>0) and (step%5 ==0):\n",
    "                agent.learn()\n",
    "\n",
    "            observation = observation_\n",
    "\n",
    "            step += 1\n",
    "\n",
    "\n",
    "        agent.save_model()\n",
    "        agent.plot_cost()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "444570fc77af2c29d522e1d411626eb7e891874e3640743a962b379aa4ecce5f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('RLpython')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
